# AI-Semantle 분석 보고서

## 1. 문제 정의: Semantle 게임

Semantle은 숨겨진 비밀 단어를 추측하는 게임입니다. 플레이어는 단어를 추측할 때마다 비밀 단어와의 **의미적 유사도**에 기반한 점수를 피드백으로 받습니다. 점수가 높을수록 추측한 단어가 비밀 단어와 의미적으로 가깝다는 뜻입니다. AI는 이 유사도 점수 피드백을 활용하여 제한된 횟수 안에 비밀 단어를 효율적으로 찾아야 합니다. 주요 과제는 방대한 영어 단어 중에서 어떤 단어를 추측해야 가장 효과적으로 정답에 접근할 수 있을지 결정하는 것입니다.

## 2. AI 접근 방식 개요

이 프로젝트는 Semantle 게임을 해결하기 위해 다음과 같은 주요 구성 요소를 결합한 강화학습 기반 접근 방식을 사용합니다:

*   **Q-러닝 (Q-Learning)**: 어떤 **단어 클러스터**에서 다음 단어를 추측하는 것이 장기적으로 가장 높은 보상(유사도 점수)을 가져올지 학습하는 핵심 의사결정 알고리즘입니다.
*   **단어 클러스터링 (Word Clustering)**: 방대한 영어 단어들을 의미적 유사성에 따라 그룹화(클러스터링)하여 AI가 탐색해야 할 단어 공간의 복잡성을 줄입니다.
*   **휴리스틱 함수 (Heuristic Function)**: Q-러닝 에이전트가 특정 클러스터를 선택하면, 그 클러스터 내에서 실제로 추측할 가장 유망한 단어를 선택하는 규칙 기반 또는 모델 기반의 함수입니다.
*   **(README 기반) 계층적 Q-에이전트 구조**: 문제의 복잡성을 더 줄이기 위해 메인 Q-에이전트가 하위 Q-에이전트(각각 특정 클러스터 그룹 담당)를 선택하는 계층적 구조를 사용했을 수 있습니다 (`qlearning.py` 구현과는 다소 차이가 있을 수 있음).

## 3. 세부 구성 요소 분석

### 3.1. 단어 임베딩 및 유사도 계산

*   **Word2Vec (Google News Dataset)**: 단어의 의미를 벡터 공간에 표현하기 위해 사전 훈련된 Google News Word2Vec 모델을 사용합니다 (`gensim` 라이브러리). 각 단어는 고차원 벡터로 표현됩니다.
*   **유사도 계산**: 두 단어 벡터 간의 코사인 유사도를 계산하여 의미적 유사성을 정량화합니다 (`gnews_model.similarity(word1, word2)`). 이 값은 0에서 1 사이이며, 100을 곱하여 Semantle 게임의 점수처럼 사용됩니다. 이것이 AI가 받는 핵심 피드백입니다.

### 3.2. 단어 클러스터링

*   **목적**: 전체 단어 사전을 직접 탐색하는 것은 비효율적이므로, 의미적으로 유사한 단어들을 묶어 탐색 공간을 축소합니다. AI는 개별 단어 대신 클러스터 단위로 탐색 전략을 세웁니다.
*   **방법**: README에서는 응집형 계층적 클러스터링(Agglomerative Hierarchical Clustering)을 사용했다고 언급합니다. `src/` 내의 `cluster.py`, `clusters.py`, `secondLevelClusters.py` 등이 이와 관련될 수 있습니다. 생성된 클러스터 정보는 `qlearning.py`에서 `from clusters import clusters` 형태로 로드되어 사용됩니다.

### 3.3. Q-러닝 기반 클러스터 선택 (`qlearning.py` 기준)

*   **상태 (State)**:
    *   현재 게임 상황을 나타냅니다. 각 클러스터별로 현재까지 얻은 **최고 유사도 점수를 특정 구간(bin)으로 변환**한 값들의 튜플입니다.
    *   `binSimilarityScore` 함수는 유사도 점수를 `[0, 5, 15, 25]` 구간 기준으로 0~4의 5개 레벨로 변환합니다.
    *   예: 클러스터 4개 -> 상태 `(1, 3, 0, 2)` (첫 클러스터 최고점 0-4점, 두 번째 15-24점 등)
    *   목적: 상태 공간의 크기를 줄여 학습 효율성을 높입니다 ("차원의 저주" 완화). 상태 공간 크기 = (구간 수)^(클러스터 수) (e.g., 5^4 = 625).
*   **행동 (Action)**:
    *   다음에 단어를 추측할 **클러스터의 인덱스**를 선택하는 것입니다.
    *   행동의 총 개수 = 클러스터의 수 (`num_clusters`).
*   **보상 (Reward)**:
    *   행동(클러스터 선택) 후 휴리스틱 함수가 선택한 단어를 추측했을 때 얻는 **실제 유사도 점수** (`similarity_score`)입니다.
*   **Q-테이블 (Q-Table)**:
    *   상태-행동 쌍에 대한 **미래 보상 기댓값 (Q-값)**을 저장하는 딕셔너리 (`q_table`).
    *   `q_table[state][action]` = 상태 `state`에서 행동 `action`(클러스터 선택)을 했을 때의 Q-값.
*   **Q-값 업데이트**:
    *   표준 Q-러닝 업데이트 공식을 사용하여 Q-테이블을 업데이트합니다 (`QAgent.update` 함수).
        \[ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \]
    *   학습률 \( \alpha = 0.3 \), 할인율 \( \gamma = 0.75 \) 사용.
*   **탐험/활용 전략**:
    *   **엡실론-탐욕 (Epsilon-Greedy)**: 확률 \( \epsilon \)으로 무작위 클러스터 선택 (탐험), \( 1-\epsilon \) 확률로 현재 상태에서 Q-값이 가장 높은 클러스터 선택 (활용).
    *   **엡실론 감쇠 (Epsilon Decay)**: 게임 진행에 따라 \( \epsilon \) 값을 1.0에서 0.01까지 점차 감소시켜 탐험 비중을 줄이고 활용 비중을 높임 (`epsilon_decay = 0.04`).

### 3.4. 휴리스틱 기반 단어 선택 (`choose_word_from_cluster` 함수)

*   **역할**: Q-에이전트가 클러스터를 선택하면, 이 함수가 해당 클러스터 내에서 **실제로 추측할 단어**를 고릅니다.
*   **탐욕적 휴리스틱 로직 (`qlearning.py` 구현)**:
    1.  이전 추측들 중 가장 높은 유사도 점수를 얻은 단어(`best_word`)와 가장 낮은 점수를 얻은 단어(`worst_word`)를 찾습니다.
    2.  클러스터 내 모든 단어에 대해 `best_word`와의 유사도(`sim_best`)와 `worst_word`와의 유사도(`sim_worst`)를 계산합니다.
    3.  만약 최고 유사도 점수(`best_similarity`)가 임계값(20)보다 낮으면, `(sim_best + sim_worst) / 2`를 사용 (넓은 탐색).
    4.  임계값보다 높으면, `sim_best - sim_worst`를 사용 (좋은 방향으로 집중).
    5.  계산된 조정된 유사도 점수가 가장 높은 단어를 선택하여 반환합니다.
*   **RNN 휴리스틱 (README 언급)**: 탐욕적 방식 대신 RNN 모델을 사용하여 다음 추측 단어의 유사도를 예측하는 실험도 수행되었습니다.

### 3.5. 학습 과정 상세

Q-러닝 에이전트는 여러 게임 에피소드를 플레이하면서 최적의 클러스터 선택 정책을 학습합니다. 학습 과정은 다음과 같습니다:

1.  **초기화**: Q-테이블을 0 또는 작은 무작위 값으로 초기화하고, 엡실론(ε) 값을 1.0으로 설정합니다.
2.  **에피소드 반복**: 지정된 게임 수 (`num_games`)만큼 다음 과정을 반복합니다.
    a.  **게임 시작**: 새로운 비밀 단어를 설정하고 환경을 초기 상태로 리셋합니다.
    b.  **스텝 반복**: 최대 추측 횟수(`max_guesses`) 또는 게임 종료 시까지 다음을 반복합니다.
        i.  **현재 상태 확인**: 현재 환경의 상태 `s`를 얻습니다 (구간화된 유사도 점수 튜플).
        ii. **행동 선택**: 엡실론-탐욕 정책에 따라 행동 `a` (클러스터 인덱스)를 선택합니다 (`choose_action`). 초기에는 무작위 탐험이 많습니다.
        iii. **단어 선택 및 추측**: 선택된 클러스터 `a`에서 휴리스틱 함수(`choose_word_from_cluster`)를 사용하여 단어 `w`를 선택하고 추측합니다 (`guess_word`).
        iv. **보상 및 다음 상태 관찰**: 추측 결과로 보상 `r` (유사도 점수)과 다음 상태 `s'`를 얻습니다.
        v.  **Q-테이블 업데이트**: 관찰된 `(s, a, r, s')` 정보를 사용하여 Q-러닝 업데이트 규칙에 따라 `Q(s, a)` 값을 업데이트합니다 (`update`). 이를 통해 특정 상태에서 특정 클러스터를 선택하는 것의 장기적 가치를 학습합니다.
        vi. **상태 전이**: 현재 상태를 `s'`로 업데이트합니다.
c.  **엡실론 감쇠**: 한 게임 에피소드가 끝나면 엡실론 값을 감소시킵니다 (`decayEpsilon`). 게임이 진행될수록 에이전트는 무작위 탐험 대신 학습된 Q-값을 더 많이 활용하게 됩니다.
3.  **학습 완료**: 모든 게임 에피소드가 완료되면 Q-테이블에는 각 상태에서 어떤 클러스터를 선택하는 것이 가장 좋은지에 대한 정보가 학습되어 있습니다.

### 3.6. 추론 과정 상세 (학습 후 게임 플레이)

학습이 완료된 Q-테이블을 사용하여 실제 Semantle 게임을 플레이하는 과정(추론)은 다음과 같습니다:

1.  **학습된 Q-테이블 로드**: 학습 과정에서 생성된 Q-테이블을 사용합니다.
2.  **엡실론 설정**: 탐험을 최소화하고 학습된 최적 정책을 활용하기 위해 엡실론(ε) 값을 매우 작게 설정합니다 (예: 0 또는 `decayEpsilon`의 최소값인 0.01).
3.  **게임 시작**: 새로운 비밀 단어를 설정하고 환경을 초기 상태로 리셋합니다.
4.  **스텝 반복**: 게임 종료 시까지 다음을 반복합니다.
    a.  **현재 상태 확인**: 현재 환경의 상태 `s`를 얻습니다.
    b.  **최적 행동 선택**: 현재 상태 `s`에서 Q-테이블을 참조하여 **가장 높은 Q-값을 가지는 행동 `a` (클러스터 인덱스)**를 선택합니다. (만약 ε > 0 이면 아주 가끔 무작위 행동 선택 가능성 있음)
    c.  **단어 선택 및 추측**: 선택된 최적 클러스터 `a`에서 휴리스틱 함수(`choose_word_from_cluster`)를 사용하여 단어 `w`를 선택하고 추측합니다 (`guess_word`).
    d.  **상태 전이**: 추측 결과에 따라 다음 상태 `s'`로 이동합니다.
5.  **게임 종료**: 비밀 단어를 맞추거나 최대 추측 횟수에 도달하면 게임이 종료됩니다.

추론 과정에서는 더 이상 Q-테이블을 업데이트하지 않고, 오직 학습된 지식을 사용하여 가장 효율적인 클러스터를 선택하는 데 집중합니다.

## 4. 전체 워크플로우 요약

1.  **(학습 단계)** 다수의 게임 플레이를 통해 Q-테이블 학습 (3.5. 학습 과정 상세 참조).
2.  **(추론 단계)** 학습된 Q-테이블과 낮은 엡실론 값을 사용하여 실제 게임 플레이 (3.6. 추론 과정 상세 참조).

## 5. 주요 결과 (README 기반)

*   탐욕적 휴리스틱을 사용한 AI는 50번의 추측 내에서 **79.2%**의 게임을 성공적으로 해결했습니다.
*   RNN 기반 휴리스틱은 성공률(50.1%)과 추측 속도 면에서 탐욕적 방식보다 성능이 낮았습니다.
*   엡실론 감쇠 전략은 효과적이어서, 10,000 게임 동안 Q-에이전트가 가능한 상태의 상당 부분(약 600/625개)을 탐색할 수 있었습니다.

이 분석은 AI-Semantle 프로젝트가 Q-러닝, 클러스터링, 휴리스틱을 효과적으로 결합하여 복잡한 자연어 기반 게임인 Semantle을 해결하는 방식을 보여줍니다.
