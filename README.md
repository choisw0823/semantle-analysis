# AI-Semantle 분석 보고서

## 1. 문제 정의: Semantle 게임

Semantle은 숨겨진 비밀 단어를 추측하는 게임입니다. 플레이어는 단어를 추측할 때마다 비밀 단어와의 **의미적 유사도**에 기반한 점수를 피드백으로 받습니다. 점수가 높을수록 추측한 단어가 비밀 단어와 의미적으로 가깝다는 뜻입니다. AI는 이 유사도 점수 피드백을 활용하여 제한된 횟수 안에 비밀 단어를 효율적으로 찾아야 합니다. 주요 과제는 방대한 영어 단어 중에서 어떤 단어를 추측해야 가장 효과적으로 정답에 접근할 수 있을지 결정하는 것입니다.

## 2. AI 접근 방식 개요

이 프로젝트는 Semantle 게임을 해결하기 위해 다음과 같은 주요 구성 요소를 결합한 강화학습 기반 접근 방식을 사용합니다:

*   **Q-러닝 (Q-Learning)**: 어떤 **단어 클러스터**에서 다음 단어를 추측하는 것이 장기적으로 가장 높은 보상(유사도 점수)을 가져올지 학습하는 핵심 의사결정 알고리즘입니다.
*   **단어 클러스터링 (Word Clustering)**: 방대한 영어 단어들을 의미적 유사성에 따라 그룹화(클러스터링)하여 AI가 탐색해야 할 단어 공간의 복잡성을 줄입니다.
*   **휴리스틱 함수 (Heuristic Function)**: Q-러닝 에이전트가 특정 클러스터를 선택하면, 그 클러스터 내에서 실제로 추측할 가장 유망한 단어를 선택하는 규칙 기반 또는 모델 기반의 함수입니다.
*   **(README 기반) 계층적 Q-에이전트 구조**: 문제의 복잡성을 더 줄이기 위해 메인 Q-에이전트가 하위 Q-에이전트(각각 특정 클러스터 그룹 담당)를 선택하는 계층적 구조를 사용했을 수 있습니다 (`qlearning.py` 구현과는 다소 차이가 있을 수 있음).

## 3. 세부 구성 요소 분석

### 3.1. 단어 임베딩 및 유사도 계산

*   **Word2Vec (Google News Dataset)**: 단어의 의미를 벡터 공간에 표현하기 위해 사전 훈련된 Google News Word2Vec 모델을 사용합니다 (`gensim` 라이브러리). 각 단어는 고차원 벡터로 표현됩니다.
*   **유사도 계산**: 두 단어 벡터 간의 코사인 유사도를 계산하여 의미적 유사성을 정량화합니다 (`gnews_model.similarity(word1, word2)`). 이 값은 0에서 1 사이이며, 100을 곱하여 Semantle 게임의 점수처럼 사용됩니다. 이것이 AI가 받는 핵심 피드백입니다.

### 3.2. 단어 클러스터링

*   **목적**: 전체 단어 사전을 직접 탐색하는 것은 매우 비효율적이므로, Word2Vec 벡터 공간에서 의미적으로 유사한 단어들을 그룹(클러스터)으로 묶어 탐색 공간을 효과적으로 축소합니다. AI는 개별 단어 대신 클러스터 단위로 탐색 및 학습 전략을 세웁니다.
*   **방법 (README 및 코드 기반 추론)**:
    *   **응집형 계층적 클러스터링 (Agglomerative Hierarchical Clustering)**: README에서 언급된 이 방식은 '상향식(bottom-up)' 접근법입니다.
        1.  초기에는 모든 단어를 각각 별개의 클러스터로 간주합니다.
        2.  가장 유사한(가까운) 두 클러스터를 반복적으로 병합합니다. 클러스터 간 유사성은 각 클러스터에 속한 단어들의 Word2Vec 벡터 간의 평균 거리, 최소 거리, 또는 최대 거리 등을 사용하여 정의할 수 있습니다.
        3.  이 병합 과정을 최종적으로 원하는 수의 클러스터가 남을 때까지 또는 특정 조건(예: 클러스터 간 거리가 일정 임계값 이상)을 만족할 때까지 반복합니다.
        4.  결과적으로 'king', 'queen', 'royalty' 같이 의미적으로 유사한 단어들이 하나의 클러스터로 묶이게 됩니다.
    *   **관련 파일 추정 역할**: `src/` 디렉토리 내의 파일들은 다음과 같은 역할을 수행했을 것으로 추정됩니다:
        *   `cluster.py` 또는 `cluster.ipynb`: 실제 클러스터링 알고리즘을 구현하고 실행하는 메인 스크립트일 가능성이 높습니다. Word2Vec 벡터를 입력받아 응집형 계층적 클러스터링(예: `scikit-learn` 라이브러리 활용)을 수행하고, 각 단어의 클러스터 할당 결과를 생성합니다.
        *   `clusters.py` 또는 `secondLevelClusters.py`: 클러스터링의 최종 결과, 즉 생성된 단어 그룹(클러스터) 목록 자체를 저장하는 파일일 수 있습니다. `qlearning.py`에서 `from clusters import clusters` 구문을 통해 직접 임포트하여 사용할 수 있도록 파이썬 리스트나 딕셔너리 형태로 저장되었을 것입니다. `secondLevelClusters.py` 파일 이름은 클러스터링이 2단계로 진행되었거나(예: 대규모 클러스터링 후 세분화) 클러스터 자체가 계층적 구조를 가질 가능성을 암시합니다.
        *   **사전 계산 (Pre-computation)**: 클러스터링은 Q-러닝 에이전트 학습이나 추론 이전에 **별도로 미리 수행되는 오프라인 작업**입니다. `qlearning.py`는 이 미리 계산된 클러스터링 결과를 로드하여 활용합니다.

### 3.3. Q-러닝 기반 클러스터 선택 (`qlearning.py` 기준)

*   **상태 (State)**:
    *   현재 게임 상황을 나타냅니다. 각 클러스터별로 현재까지 얻은 **최고 유사도 점수를 특정 구간(bin)으로 변환**한 값들의 튜플입니다.
    *   `binSimilarityScore` 함수는 유사도 점수를 `[0, 5, 15, 25]` 구간 기준으로 0~4의 5개 레벨로 변환합니다.
    *   예: 클러스터 4개 -> 상태 `(1, 3, 0, 2)` (첫 클러스터 최고점 0-4점, 두 번째 15-24점 등)
    *   목적: 상태 공간의 크기를 줄여 학습 효율성을 높입니다 ("차원의 저주" 완화). 상태 공간 크기 = (구간 수)^(클러스터 수) (e.g., 5^4 = 625).
*   **행동 (Action)**:
    *   다음에 단어를 추측할 **클러스터의 인덱스**를 선택하는 것입니다.
    *   행동의 총 개수 = 클러스터의 수 (`num_clusters`).
*   **보상 (Reward)**:
    *   행동(클러스터 선택) 후 휴리스틱 함수가 선택한 단어를 추측했을 때 얻는 **실제 유사도 점수** (`similarity_score`)입니다.
*   **Q-테이블 (Q-Table)**:
    *   상태-행동 쌍에 대한 **미래 보상 기댓값 (Q-값)**을 저장하는 딕셔너리 (`q_table`).
    *   `q_table[state][action]` = 상태 `state`에서 행동 `action`(클러스터 선택)을 했을 때의 Q-값.
*   **Q-값 업데이트**:
    *   표준 Q-러닝 업데이트 공식을 사용하여 Q-테이블을 업데이트합니다 (`QAgent.update` 함수).
        \[ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] \]
    *   학습률 \( \alpha = 0.3 \), 할인율 \( \gamma = 0.75 \) 사용.
*   **탐험/활용 전략**:
    *   **엡실론-탐욕 (Epsilon-Greedy)**: 확률 \( \epsilon \)으로 무작위 클러스터 선택 (탐험), \( 1-\epsilon \) 확률로 현재 상태에서 Q-값이 가장 높은 클러스터 선택 (활용).
    *   **엡실론 감쇠 (Epsilon Decay)**: 게임 진행에 따라 \( \epsilon \) 값을 1.0에서 0.01까지 점차 감소시켜 탐험 비중을 줄이고 활용 비중을 높임 (`epsilon_decay = 0.04`).

### 3.4. 휴리스틱 기반 단어 선택 (`choose_word_from_cluster` 함수)

*   **역할**: Q-에이전트가 클러스터를 선택하면, 이 함수가 해당 클러스터 내에서 **실제로 추측할 단어**를 고릅니다.
*   **탐욕적 휴리스틱 로직 (`qlearning.py` 구현)**:
    1.  이전 추측들 중 가장 높은 유사도 점수를 얻은 단어(`best_word`)와 가장 낮은 점수를 얻은 단어(`worst_word`)를 찾습니다.
    2.  클러스터 내 모든 단어에 대해 `best_word`와의 유사도(`sim_best`)와 `worst_word`와의 유사도(`sim_worst`)를 계산합니다.
    3.  만약 최고 유사도 점수(`best_similarity`)가 임계값(20)보다 낮으면, `(sim_best + sim_worst) / 2`를 사용 (넓은 탐색).
    4.  임계값보다 높으면, `sim_best - sim_worst`를 사용 (좋은 방향으로 집중).
    5.  계산된 조정된 유사도 점수가 가장 높은 단어를 선택하여 반환합니다.
*   **RNN 휴리스틱 (README 언급)**: 탐욕적 방식 대신 RNN 모델을 사용하여 다음 추측 단어의 유사도를 예측하는 실험도 수행되었습니다.

### 3.5. 학습 과정 상세

Q-러닝 에이전트는 여러 게임 에피소드를 플레이하면서 최적의 클러스터 선택 정책을 학습합니다. 학습 과정은 다음과 같습니다:

1.  **초기화**: Q-테이블을 0 또는 작은 무작위 값으로 초기화하고, 엡실론(ε) 값을 1.0으로 설정합니다.
2.  **에피소드 반복**: 지정된 게임 수 (`num_games`)만큼 다음 과정을 반복합니다.
    a.  **게임 시작**: 새로운 비밀 단어를 설정하고 환경을 초기 상태로 리셋합니다.
    b.  **스텝 반복**: 최대 추측 횟수(`max_guesses`) 또는 게임 종료 시까지 다음을 반복합니다.
        i.  **현재 상태 확인**: 현재 환경의 상태 `s`를 얻습니다 (구간화된 유사도 점수 튜플).
        ii. **행동 선택**: 엡실론-탐욕 정책에 따라 행동 `a` (클러스터 인덱스)를 선택합니다 (`choose_action`). 초기에는 무작위 탐험이 많습니다.
        iii. **단어 선택 및 추측**: 선택된 클러스터 `a`에서 휴리스틱 함수(`choose_word_from_cluster`)를 사용하여 단어 `w`를 선택하고 추측합니다 (`guess_word`).
        iv. **보상 및 다음 상태 관찰**: 추측 결과로 보상 `r` (유사도 점수)과 다음 상태 `s'`를 얻습니다.
        v.  **Q-테이블 업데이트**: 관찰된 `(s, a, r, s')` 정보를 사용하여 Q-러닝 업데이트 규칙에 따라 `Q(s, a)` 값을 업데이트합니다 (`update`). 이를 통해 특정 상태에서 특정 클러스터를 선택하는 것의 장기적 가치를 학습합니다.
        vi. **상태 전이**: 현재 상태를 `s'`로 업데이트합니다.
c.  **엡실론 감쇠**: 한 게임 에피소드가 끝나면 엡실론 값을 감소시킵니다 (`decayEpsilon`). 게임이 진행될수록 에이전트는 무작위 탐험 대신 학습된 Q-값을 더 많이 활용하게 됩니다.
3.  **학습 완료**: 모든 게임 에피소드가 완료되면 Q-테이블에는 각 상태에서 어떤 클러스터를 선택하는 것이 가장 좋은지에 대한 정보가 학습되어 있습니다.

### 3.6. 추론 과정 상세 (학습 후 게임 플레이)

학습이 완료된 Q-테이블을 사용하여 실제 Semantle 게임을 플레이하는 과정(추론)은 다음과 같습니다:

1.  **학습된 Q-테이블 로드**: 학습 과정에서 생성된 Q-테이블을 사용합니다.
2.  **엡실론 설정**: 탐험을 최소화하고 학습된 최적 정책을 활용하기 위해 엡실론(ε) 값을 매우 작게 설정합니다 (예: 0 또는 `decayEpsilon`의 최소값인 0.01).
3.  **게임 시작**: 새로운 비밀 단어를 설정하고 환경을 초기 상태로 리셋합니다.
4.  **스텝 반복**: 게임 종료 시까지 다음을 반복합니다.
    a.  **현재 상태 확인**: 현재 환경의 상태 `s`를 얻습니다.
    b.  **최적 행동 선택**: 현재 상태 `s`에서 Q-테이블을 참조하여 **가장 높은 Q-값을 가지는 행동 `a` (클러스터 인덱스)**를 선택합니다. (만약 ε > 0 이면 아주 가끔 무작위 행동 선택 가능성 있음)
    c.  **단어 선택 및 추측**: 선택된 최적 클러스터 `a`에서 휴리스틱 함수(`choose_word_from_cluster`)를 사용하여 단어 `w`를 선택하고 추측합니다 (`guess_word`).
    d.  **상태 전이**: 추측 결과에 따라 다음 상태 `s'`로 이동합니다.
5.  **게임 종료**: 비밀 단어를 맞추거나 최대 추측 횟수에 도달하면 게임이 종료됩니다.

추론 과정에서는 더 이상 Q-테이블을 업데이트하지 않고, 오직 학습된 지식을 사용하여 가장 효율적인 클러스터를 선택하는 데 집중합니다.

## 4. 전체 워크플로우 요약

1.  **(학습 단계)** 다수의 게임 플레이를 통해 Q-테이블 학습 (3.5. 학습 과정 상세 참조).
2.  **(추론 단계)** 학습된 Q-테이블과 낮은 엡실론 값을 사용하여 실제 게임 플레이 (3.6. 추론 과정 상세 참조).

## 5. 주요 결과 (README 기반)

*   탐욕적 휴리스틱을 사용한 AI는 50번의 추측 내에서 **79.2%**의 게임을 성공적으로 해결했습니다.
*   RNN 기반 휴리스틱은 성공률(50.1%)과 추측 속도 면에서 탐욕적 방식보다 성능이 낮았습니다.
*   엡실론 감쇠 전략은 효과적이어서, 10,000 게임 동안 Q-에이전트가 가능한 상태의 상당 부분(약 600/625개)을 탐색할 수 있었습니다.

## 6. README와 qlearning.py 구현 간의 잠재적 차이점

분석 과정에서 `README.md`의 설명과 `qlearning.py`의 실제 코드 구현 사이에 다음과 같은 잠재적 차이점이 관찰되었습니다:

*   **계층적 Q-에이전트 구조**: `README.md`는 메인 에이전트가 클러스터 그룹을 담당하는 하위 에이전트를 선택하는 "계층적 Q-에이전트" 구조를 언급합니다. 이는 문제의 복잡성을 줄이는 데 기여한다고 설명합니다. 하지만, `qlearning.py` 파일 내에는 이러한 계층적 구조가 명확히 구현되어 있지 않고, **단일 `QAgent`**가 전체 클러스터 세트에서 직접 행동(클러스터 인덱스)을 선택하는 방식으로 구현되어 있습니다. 이는 `qlearning.py`가 프로젝트의 특정 버전 또는 단순화된 접근 방식을 나타낼 수 있음을 시사합니다.

*   **휴리스틱 함수 사용**: `README.md`는 탐욕적 휴리스틱과 RNN 휴리스틱 두 가지를 모두 언급하고 결과를 비교합니다. 그러나 `qlearning.py`의 핵심 학습 루프는 명시적으로 `choose_word_from_cluster`라는 **탐욕적 휴리스틱 함수만 사용**하고 있습니다. RNN 휴리스틱은 `heuristicrnn.py` 또는 관련 Jupyter Notebook 파일에서 별도로 구현 및 실험되었을 가능성이 높습니다.

*   **클러스터링 과정**: `README.md`는 클러스터링 방법으로 '응집형 계층적 클러스터링'을 구체적으로 명시합니다. 반면, `qlearning.py`는 `from clusters import clusters` 구문을 통해 **사전에 생성된 클러스터 데이터**를 단순히 로드하여 사용합니다. 즉, `qlearning.py` 스크립트 자체에는 클러스터링 알고리즘의 구현이 포함되어 있지 않습니다.

이러한 차이점들은 `README.md`가 프로젝트의 전체적인 설계나 다양한 실험 버전을 포괄적으로 설명하는 반면, `qlearning.py`는 특정 시점의 구체적인 구현(특히 탐욕적 휴리스틱을 사용한 단일 Q-에이전트 학습)을 보여주기 때문에 발생할 수 있습니다.

이 분석은 AI-Semantle 프로젝트가 Q-러닝, 클러스터링, 휴리스틱을 효과적으로 결합하여 복잡한 자연어 기반 게임인 Semantle을 해결하는 방식을 보여줍니다.
